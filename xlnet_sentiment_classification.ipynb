{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "└ @ CUDA /home/ec2-user/.julia/packages/CUDA/gKMm0/src/state.jl:251\n"
     ]
    }
   ],
   "source": [
    "include(\"./XLNet.jl\")\n",
    "using PyCall\n",
    "using DelimitedFiles\n",
    "using JLD2\n",
    "using Knet\n",
    "using .XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = xlnet_base_hparams #xlnet_base_params are defined inside XLNet Module\n",
    "\n",
    " #Set sequance length and batch size accoring to your GPU\n",
    "SEQ_LEN = 340\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "#this hparam stands for how many layers to freeze from beggining.\n",
    "#You may play with this hparam according to your GPU memory. (There are tottaly 12 layers)\n",
    "hparams[\"n_freeze\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "For this example, we will work on sentiment classification on IMDB dataset (https://ai.stanford.edu/~amaas/data/sentiment/). Dataset can be downloaded as follows:\n",
    "\n",
    "```$ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz```  \n",
    "```$ tar -xf aclImdb_v1.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ec2-user/aclImdb\"\n",
    "\n",
    "#Read Train Data\n",
    "pos_train_files = readdir(dataset_path * \"/train/pos\")\n",
    "neg_train_files = readdir(dataset_path * \"/train/neg\")\n",
    "n_train = length(pos_train_files) + length(neg_train_files)\n",
    "n_classes = 2\n",
    "xtrn,ytrn = [],[]\n",
    "\n",
    "for filename in pos_train_files\n",
    "    s = open( dataset_path * \"/train/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_train_files\n",
    "    s = open( dataset_path * \"/train/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"neg\")\n",
    "end\n",
    "\n",
    "#Read Test Data\n",
    "pos_test_files = readdir(dataset_path * \"/test/pos\")\n",
    "neg_test_files = readdir(dataset_path * \"/test/neg\")\n",
    "n_train = length(pos_test_files) + length(neg_test_files)\n",
    "xtst,ytst = [],[]\n",
    "\n",
    "for filename in pos_test_files\n",
    "    s = open( dataset_path * \"/test/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_test_files\n",
    "    s = open( dataset_path * \"/test/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"neg\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function converts given text sample to token ids, adjust its length based on the sequance length and\n",
    "adds special tokens at the end of the sample.\n",
    "=#\n",
    "\n",
    "function prepare_sample( text, seq_len, sp )\n",
    "  input_ids = sp.encode_as_ids( text )\n",
    "  input_length = length(input_ids)\n",
    "\n",
    "  if( input_length > seq_len - 2 ); input_length = seq_len - 2; end\n",
    "  input_ids = input_ids[1:input_length]\n",
    "  \n",
    "  push!(input_ids, specaialTokens[\"<SEP>\"] )\n",
    "  push!(input_ids, specaialTokens[\"<CLS>\"] )\n",
    "\n",
    "  attn_mask = zeros(Int32, seq_len)\n",
    "  padded_input_ids = zeros(Int32, seq_len)\n",
    "  \n",
    "  attn_mask[1:input_length + 2] .= 1\n",
    "  attn_mask = 1 .- attn_mask\n",
    "  padded_input_ids[1:input_length + 2] = input_ids\n",
    "  (padded_input_ids, attn_mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:25/00:25, 983.28i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 857608.31i/s] \n",
      "┣                    ┫ [0.00%, 1/25000, 00:00/00:00, 56837.56i/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:26/00:26, 964.06i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 838001.59i/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6250-element Knet.Train20.Data{Tuple{Array{Array{Int64,1},N} where N,Array{Int64,N} where N}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to use sentencepice tokenizer from pycall\n",
    "#sentencepiece tokenizer : https://github.com/google/sentencepiece\n",
    "\n",
    "spm = pyimport(\"sentencepiece\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"/home/ec2-user/project/checkpoint/spiece.model\")\n",
    "\n",
    "println(\"prepearing train data...\")\n",
    "xtrn = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtrn) ]\n",
    "ytrn = [ y==\"pos\" ? 2 : 1 for y in progress(ytrn) ]\n",
    "\n",
    "trn_token_ids = [x[1] for x in xtrn ]\n",
    "trn_attn_masks= [x[2] for x in xtrn ]\n",
    "xtrn = permutedims( hcat(trn_token_ids, trn_attn_masks ) , [2,1] )\n",
    "\n",
    "println(\"prepearing test data...\")\n",
    "xtst = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtst) ]\n",
    "ytst = [ y == \"pos\" ? 2 : 1 for y in progress(ytst) ]\n",
    "\n",
    "tst_token_ids = [x[1] for x in xtst ]\n",
    "tst_attn_masks= [x[2] for x in xtst ]\n",
    "xtst = permutedims( hcat(tst_token_ids, tst_attn_masks ), [2,1] )\n",
    "\n",
    "dtrn = minibatch( xtrn, ytrn ,BATCH_SIZE, shuffle=true )\n",
    "dtst = minibatch( xtst, ytst ,BATCH_SIZE, shuffle=true )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define XLNet Model\n",
    "\n",
    "To be able to run finetuning, you need to download pretrained weights. I have prepared pretrained weights in JLD2 format.  \n",
    "## TODO: Add weights and tokenizer files to drive and give link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 18 entries:\n",
       "  \"layer_0\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_5\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_1\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_11\" => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_4\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_9\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"mask_emb\" => Float32[1.0, 0.0, 1.0]\n",
       "  \"seg_emb\"  => Float32[-0.089485 0.00394292; -0.0242453 0.0256528; … ; -0.0594…\n",
       "  \"layer_7\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"r_r_bias\" => Float32[-0.0875984 0.117815 … -0.0863528 0.0588675; 0.326463 0.…\n",
       "  \"layer_8\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_6\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"r_w_bias\" => Float32[-0.0260591 0.0745491 … -0.12565 -0.0483745; 0.209319 0.…\n",
       "  \"word_emb\" => Float32[-0.00489193 0.0655301 … -0.00614611 0.034621; 0.0380875…\n",
       "  \"layer_2\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"layer_10\" => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…\n",
       "  \"r_s_bias\" => Float32[-0.277643 0.141542 … -0.168406 -0.287923; -0.212232 0.2…\n",
       "  \"layer_3\"  => Dict{Any,Any}(\"ff\"=>Dict{Any,Any}(\"layer_1\"=>Dict{Any,Any}(\"ker…"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Weights\n",
    "@load \"/home/ec2-user/project/checkpoint/weights_base.jld2\" weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier model created\n"
     ]
    }
   ],
   "source": [
    "model = create_xlnet_model( hparams, weights )\n",
    "classifier = XLNetClassifier( hparams[\"d_model\"], n_classes , model )\n",
    "println(\"classifier model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: accuracy(model,data; o...) is deprecated, please use accuracy(model; data=data, o...)\n",
      "└ @ Knet.Ops20 /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:205\n",
      "┣▎                   ┫ [1.86%, 116/6250, 17:06/15:21:43, 8.52s/i] "
     ]
    }
   ],
   "source": [
    "acc = accuracy( classifier ,  progress( dtst ) )\n",
    "println(\"accuracy : \" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
