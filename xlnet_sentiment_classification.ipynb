{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "└ @ CUDA /home/ec2-user/.julia/packages/CUDA/gKMm0/src/state.jl:251\n"
     ]
    }
   ],
   "source": [
    "include(\"./XLNet.jl\")\n",
    "using PyCall\n",
    "using DelimitedFiles\n",
    "using JLD2\n",
    "using Knet\n",
    "using Random\n",
    "using .XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = xlnet_base_hparams #xlnet_base_params are defined inside XLNet Module\n",
    "\n",
    "#Set sequance length and batch size accoring to your GPU\n",
    "SEQ_LEN = 340\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "#this hparam stands for how many layers to freeze from beggining.\n",
    "#You may play with this hparam according to your GPU memory. (There are tottaly 12 layers)\n",
    "hparams[\"n_freeze\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "For this example, we will work on sentiment classification on IMDB dataset (https://ai.stanford.edu/~amaas/data/sentiment/). Dataset can be downloaded as follows:\n",
    "\n",
    "```$ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz```  \n",
    "```$ tar -xf aclImdb_v1.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ec2-user/aclImdb\"\n",
    "\n",
    "#Read Train Data\n",
    "pos_train_files = readdir(dataset_path * \"/train/pos\")\n",
    "neg_train_files = readdir(dataset_path * \"/train/neg\")\n",
    "n_train = length(pos_train_files) + length(neg_train_files)\n",
    "n_classes = 2 #classes\n",
    "xtrn,ytrn = [],[]\n",
    "\n",
    "for filename in pos_train_files\n",
    "    s = open( dataset_path * \"/train/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_train_files\n",
    "    s = open( dataset_path * \"/train/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"neg\")\n",
    "end\n",
    "\n",
    "#Read Test Data\n",
    "pos_test_files = readdir(dataset_path * \"/test/pos\")\n",
    "neg_test_files = readdir(dataset_path * \"/test/neg\")\n",
    "n_train = length(pos_test_files) + length(neg_test_files)\n",
    "xtst,ytst = [],[]\n",
    "\n",
    "for filename in pos_test_files\n",
    "    s = open( dataset_path * \"/test/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_test_files\n",
    "    s = open( dataset_path * \"/test/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"neg\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_sample (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=\n",
    "This function converts given text sample to token ids, adjust its length based on the sequance length and\n",
    "adds special tokens at the end of the sample.\n",
    "=#\n",
    "\n",
    "function prepare_sample( text, seq_len, sp )\n",
    "  input_ids = sp.encode_as_ids( text )\n",
    "  input_length = length(input_ids)\n",
    "\n",
    "  if( input_length > seq_len - 2 ); input_length = seq_len - 2; end\n",
    "  input_ids = input_ids[1:input_length]\n",
    "  \n",
    "  push!(input_ids, specaialTokens[\"<SEP>\"] )\n",
    "  push!(input_ids, specaialTokens[\"<CLS>\"] )\n",
    "\n",
    "  attn_mask = zeros(Int32, seq_len)\n",
    "  padded_input_ids = zeros(Int32, seq_len)\n",
    "  \n",
    "  attn_mask[1:input_length + 2] .= 1\n",
    "  attn_mask = 1 .- attn_mask\n",
    "  padded_input_ids[1:input_length + 2] = input_ids\n",
    "  (padded_input_ids, attn_mask)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:23/00:23, 1076.48i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 884708.47i/s] \n",
      "┣                    ┫ [0.00%, 1/25000, 00:00/00:00, 60830.95i/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:24/00:24, 1053.16i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 866233.82i/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000-element Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to use sentencepice tokenizer from pycall\n",
    "#sentencepiece tokenizer : https://github.com/google/sentencepiece\n",
    "\n",
    "spm = pyimport(\"sentencepiece\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"/home/ec2-user/project/checkpoint/spiece.model\")\n",
    "\n",
    "println(\"prepearing train data...\")\n",
    "xtrn = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtrn) ]\n",
    "ytrn = [ y==\"pos\" ? 2 : 1 for y in progress(ytrn) ]\n",
    "\n",
    "trn_token_ids = hcat( [x[1] for x in xtrn ]... )\n",
    "trn_attn_masks= hcat( [x[2] for x in xtrn ]... )\n",
    "xtrn = cat(trn_token_ids,trn_attn_masks,dims=3)\n",
    "xtrn = permutedims( xtrn, [1,3,2] )\n",
    "\n",
    "println(\"prepearing test data...\")\n",
    "xtst = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtst) ]\n",
    "ytst = [ y == \"pos\" ? 2 : 1 for y in progress(ytst) ]\n",
    "\n",
    "tst_token_ids = hcat( [x[1] for x in xtst ]... )\n",
    "tst_attn_masks= hcat( [x[2] for x in xtst ]... )\n",
    "xtst = cat(tst_token_ids,tst_attn_masks,dims=3)\n",
    "xtst = permutedims( xtst, [1,3,2] )\n",
    "\n",
    "#Split Validation\n",
    "order = shuffle( collect(1:n_train) )\n",
    "xtrn = xtrn[:,:,order]\n",
    "ytrn = ytrn[order]\n",
    "\n",
    "\n",
    "nval = 2000\n",
    "xval = xtrn[:,:,1:nval]\n",
    "yval = ytrn[1:nval]\n",
    "xtrn = xtrn[:,:,nval+1:end]\n",
    "ytrn = ytrn[nval+1:end]\n",
    "\n",
    "dtrn = minibatch( xtrn, ytrn ,BATCH_SIZE, shuffle=true )\n",
    "dval = minibatch( xval, yval ,BATCH_SIZE, shuffle=true )\n",
    "dtst = minibatch( xtst, ytst ,BATCH_SIZE, shuffle=true )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define XLNet Model\n",
    "\n",
    "To be able to run finetuning, you need to download pretrained weights. I have prepared pretrained weights in JLD2 format.  \n",
    "## TODO: Add weights and tokenizer files to drive and give link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier model created\n"
     ]
    }
   ],
   "source": [
    "#Load Weights;\n",
    "@load \"/home/ec2-user/project/checkpoint/weights_base.jld2\" weights\n",
    "model = create_xlnet_model( hparams, weights )\n",
    "classifier = XLNetClassifier( hparams[\"d_model\"], n_classes , model )\n",
    "println(\"classifier model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training !\n",
    "\n",
    "Here we will train for 3 epoch, and save the best performing weights on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣                    ┫ [0.40%, 92/23000, 08:40/36:05:45, 5.41s/i]  "
     ]
    }
   ],
   "source": [
    "trainer = adam( classifier, dtrn , lr = 1e-5 , eps=1e-8 )\n",
    "best_acc = 0\n",
    "for i=1:3\n",
    "    println(\"training epoch \",i)\n",
    "    progress!(trainer)\n",
    "    acc = acc = accuracy( classifier ,  progress( dval ) )\n",
    "    if( acc > best_acc )\n",
    "        best_acc = acc\n",
    "        save(\"best.jld2\", classifier ) #pretty easy saving :)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test !\n",
    "\n",
    "Load the saved weights  and test on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "┣                    ┫ [0.00%, 1/25000, 00:00/00:12, 2120.52i/s] "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] reshape at ./reshapedarray.jl:51 [inlined]",
      " [2] reshape at ./reshapedarray.jl:116 [inlined]",
      " [3] bmm(::Array{Float32,4}, ::Array{Float32,4}; transA::Bool, transB::Bool) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/bmm.jl:23",
      " [4] bmm at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/bmm.jl:16 [inlined]",
      " [5] einsum_4d_v1(::Array{Float32,4}, ::Array{Float32,4}) at /home/ec2-user/xlnet.jl/XLNet.jl:229",
      " [6] rel_attn_core(::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Nothing, ::Nothing, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Array{Float32,4}, ::Float32, ::Float64) at /home/ec2-user/xlnet.jl/XLNet.jl:301",
      " [7] rel_attn_core(::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Nothing, ::Nothing, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Array{Float32,4}, ::Float32) at /home/ec2-user/xlnet.jl/XLNet.jl:298",
      " [8] (::Main.XLNet.AttnLayer)(::Array{Float32,3}, ::Array{Float32,3}, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Nothing, ::Nothing, ::Array{Float32,4}, ::Nothing, ::Int64, ::Int64, ::Int64, ::Float64, ::Float64, ::Int64) at /home/ec2-user/xlnet.jl/XLNet.jl:595",
      " [9] (::XLNetModel)(::Array{Int64,2}, ::Nothing, ::Array{Int64,2}; mems::Nothing, perm_mask::Nothing, target_mapping::Nothing, inp_q::Nothing, attn_type::String, dtype::Type{T} where T) at /home/ec2-user/xlnet.jl/XLNet.jl:518",
      " [10] (::XLNetModel)(::Array{Int64,2}, ::Nothing, ::Array{Int64,2}) at /home/ec2-user/xlnet.jl/XLNet.jl:415",
      " [11] (::XLNetClassifier)(::Array{Int64,3}) at /home/ec2-user/xlnet.jl/XLNet.jl:706",
      " [12] accuracy(::XLNetClassifier; data::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}, dims::Int64, average::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:188",
      " [13] accuracy(::XLNetClassifier, ::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:206",
      " [14] accuracy(::XLNetClassifier, ::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:205",
      " [15] top-level scope at In[10]:2",
      " [16] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [17] execute_code(::String, ::String) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [18] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [19] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [20] invokelatest at ./essentials.jl:709 [inlined]",
      " [21] eventloop(::ZMQ.Socket) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [22] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "classifier = XLNetClassifier(\"best.jld2\") #Load model from saved path\n",
    "acc = accuracy( classifier ,  progress( dtst ) )\n",
    "println(\"accuracy : \" , acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
