{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"./XLNet.jl\")\n",
    "using PyCall\n",
    "using DelimitedFiles\n",
    "using JLD2\n",
    "using Knet\n",
    "using Random\n",
    "using .XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = xlnet_base_hparams #xlnet_base_params are defined inside XLNet Module\n",
    "\n",
    "#Set sequance length and batch size accoring to your GPU\n",
    "SEQ_LEN = 340\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "#this hparam stands for how many layers to freeze from beggining.\n",
    "#You may play with this hparam according to your GPU memory. (There are tottaly 12 layers)\n",
    "hparams[\"n_freeze\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "For this example, we will work on sentiment classification on IMDB dataset (https://ai.stanford.edu/~amaas/data/sentiment/). Dataset can be downloaded as follows:\n",
    "\n",
    "```$ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz```  \n",
    "```$ tar -xf aclImdb_v1.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ec2-user/aclImdb\"\n",
    "\n",
    "#Read Train Data\n",
    "pos_train_files = readdir(dataset_path * \"/train/pos\")\n",
    "neg_train_files = readdir(dataset_path * \"/train/neg\")\n",
    "n_train = length(pos_train_files) + length(neg_train_files)\n",
    "n_classes = 2\n",
    "xtrn,ytrn = [],[]\n",
    "\n",
    "for filename in pos_train_files\n",
    "    s = open( dataset_path * \"/train/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_train_files\n",
    "    s = open( dataset_path * \"/train/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"neg\")\n",
    "end\n",
    "\n",
    "#Read Test Data\n",
    "pos_test_files = readdir(dataset_path * \"/test/pos\")\n",
    "neg_test_files = readdir(dataset_path * \"/test/neg\")\n",
    "n_train = length(pos_test_files) + length(neg_test_files)\n",
    "xtst,ytst = [],[]\n",
    "\n",
    "for filename in pos_test_files\n",
    "    s = open( dataset_path * \"/test/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_test_files\n",
    "    s = open( dataset_path * \"/test/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"neg\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "This function converts given text sample to token ids, adjust its length based on the sequance length and\n",
    "adds special tokens at the end of the sample.\n",
    "=#\n",
    "\n",
    "function prepare_sample( text, seq_len, sp )\n",
    "  input_ids = sp.encode_as_ids( text )\n",
    "  input_length = length(input_ids)\n",
    "\n",
    "  if( input_length > seq_len - 2 ); input_length = seq_len - 2; end\n",
    "  input_ids = input_ids[1:input_length]\n",
    "  \n",
    "  push!(input_ids, specaialTokens[\"<SEP>\"] )\n",
    "  push!(input_ids, specaialTokens[\"<CLS>\"] )\n",
    "\n",
    "  attn_mask = zeros(Int32, seq_len)\n",
    "  padded_input_ids = zeros(Int32, seq_len)\n",
    "  \n",
    "  attn_mask[1:input_length + 2] .= 1\n",
    "  attn_mask = 1 .- attn_mask\n",
    "  padded_input_ids[1:input_length + 2] = input_ids\n",
    "  (padded_input_ids, attn_mask)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to use sentencepice tokenizer from pycall\n",
    "#sentencepiece tokenizer : https://github.com/google/sentencepiece\n",
    "\n",
    "spm = pyimport(\"sentencepiece\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"/home/ec2-user/project/checkpoint/spiece.model\")\n",
    "\n",
    "println(\"prepearing train data...\")\n",
    "xtrn = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtrn) ]\n",
    "ytrn = [ y==\"pos\" ? 2 : 1 for y in progress(ytrn) ]\n",
    "\n",
    "trn_token_ids = hcat( [x[1] for x in xtrn ]... )\n",
    "trn_attn_masks= hcat( [x[2] for x in xtrn ]... )\n",
    "xtrn = cat(trn_token_ids,trn_attn_masks,dims=3)\n",
    "xtrn = permutedims( xtrn, [1,3,2] )\n",
    "\n",
    "println(\"prepearing test data...\")\n",
    "xtst = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtst) ]\n",
    "ytst = [ y == \"pos\" ? 2 : 1 for y in progress(ytst) ]\n",
    "\n",
    "tst_token_ids = hcat( [x[1] for x in xtst ]... )\n",
    "tst_attn_masks= hcat( [x[2] for x in xtst ]... )\n",
    "xtst = cat(tst_token_ids,tst_attn_masks,dims=3)\n",
    "xtst = permutedims( xtst, [1,3,2] )\n",
    "\n",
    "#Split Validation\n",
    "order = shuffle( collect(1:n_train) )\n",
    "xtrn = xtrn[:,:,order]\n",
    "ytrn = ytrn[:,:,order]\n",
    "\n",
    "#=\n",
    "nval = 2000\n",
    "xval = xtrn[:,:,1:nval]\n",
    "yval = ytrn[:,:,1:nval]\n",
    "xtrn = xtrn[:,:,nval+1:end]\n",
    "ytrn = ytrn[:,:,nval+1:end]\n",
    "=#\n",
    "\n",
    "xval = xtrn[:,:,1:10]\n",
    "yval = ytrn[:,:,1:10]\n",
    "xtrn = xtrn[:,:,20:30]\n",
    "ytrn = ytrn[:,:,20:30]\n",
    "\n",
    "dtrn = minibatch( xtrn, ytrn ,BATCH_SIZE, shuffle=true )\n",
    "dval = minibatch( xval, yval ,BATCH_SIZE, shuffle=true )\n",
    "dtst = minibatch( xtst, ytst ,BATCH_SIZE, shuffle=true )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define XLNet Model\n",
    "\n",
    "To be able to run finetuning, you need to download pretrained weights. I have prepared pretrained weights in JLD2 format.  \n",
    "## TODO: Add weights and tokenizer files to drive and give link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Weights;\n",
    "@load \"/home/ec2-user/project/checkpoint/weights_base.jld2\" weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_xlnet_model( hparams, weights )\n",
    "classifier = XLNetClassifier( hparams[\"d_model\"], 2 , model )\n",
    "println(\"classifier model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"../classifier.jld2\", classifier )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = XLNetClassifier(\"../classifier.jld2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: accuracy(model,data; o...) is deprecated, please use accuracy(model; data=data, o...)\n",
      "└ @ Knet.Ops20 /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:205\n",
      "┣▏                   ┫ [1.10%, 276/25000, 13:22/20:10:04, 2.87s/i] "
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] reshape at ./reshapedarray.jl:51 [inlined]",
      " [2] reshape at ./reshapedarray.jl:116 [inlined]",
      " [3] rel_shift(::Array{Float32,4}, ::Int64) at /home/ec2-user/xlnet.jl/XLNet.jl:214",
      " [4] rel_attn_core(::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Nothing, ::Nothing, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Array{Float32,4}, ::Float32, ::Float64) at /home/ec2-user/xlnet.jl/XLNet.jl:303",
      " [5] rel_attn_core(::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Array{Float32,4}, ::Nothing, ::Nothing, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Array{Float32,4}, ::Float32) at /home/ec2-user/xlnet.jl/XLNet.jl:298",
      " [6] (::Main.XLNet.AttnLayer)(::Array{Float32,3}, ::Array{Float32,3}, ::Array{Float32,2}, ::Array{Float32,2}, ::Nothing, ::Nothing, ::Nothing, ::Array{Float32,4}, ::Nothing, ::Int64, ::Int64, ::Int64, ::Float64, ::Float64, ::Int64) at /home/ec2-user/xlnet.jl/XLNet.jl:595",
      " [7] (::XLNetModel)(::Array{Int64,2}, ::Nothing, ::Array{Int64,2}; mems::Nothing, perm_mask::Nothing, target_mapping::Nothing, inp_q::Nothing, attn_type::String, dtype::Type{T} where T) at /home/ec2-user/xlnet.jl/XLNet.jl:518",
      " [8] (::XLNetModel)(::Array{Int64,2}, ::Nothing, ::Array{Int64,2}) at /home/ec2-user/xlnet.jl/XLNet.jl:415",
      " [9] (::XLNetClassifier)(::Array{Int64,3}) at /home/ec2-user/xlnet.jl/XLNet.jl:706",
      " [10] accuracy(::XLNetClassifier; data::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}, dims::Int64, average::Bool, o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:188",
      " [11] accuracy(::XLNetClassifier, ::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}; o::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:206",
      " [12] accuracy(::XLNetClassifier, ::Knet.Train20.Progress{Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}}) at /home/ec2-user/.julia/packages/Knet/C0PoK/src/ops20/loss.jl:205",
      " [13] top-level scope at In[10]:1",
      " [14] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091",
      " [15] execute_code(::String, ::String) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:27",
      " [16] execute_request(::ZMQ.Socket, ::IJulia.Msg) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/execute_request.jl:86",
      " [17] #invokelatest#1 at ./essentials.jl:710 [inlined]",
      " [18] invokelatest at ./essentials.jl:709 [inlined]",
      " [19] eventloop(::ZMQ.Socket) at /home/ec2-user/.julia/packages/IJulia/rWZ9e/src/eventloop.jl:8",
      " [20] (::IJulia.var\"#15#18\")() at ./task.jl:356"
     ]
    }
   ],
   "source": [
    "acc = accuracy( classifier ,  progress( dtst ) )\n",
    "println(\"accuracy : \" , acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training !\n",
    "\n",
    "Here we will train for 3 epoch, and save the best performing weights on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣                    ┫ [0.08%, 21/25000, 02:21/46:46:16, 5.49s/i]  "
     ]
    }
   ],
   "source": [
    "trainer = adam( classifier, dtrn , lr = 1e-5 , eps=1e-8 )\n",
    "best_acc = 0\n",
    "for i=1:5\n",
    "    progress!(trainer)\n",
    "    acc = acc = accuracy( classifier ,  progress( dval ) )\n",
    "    if( acc > best_acc )\n",
    "        best_acc = acc\n",
    "        save(\"best.jld2\", classifier ) #pretty easy saving :)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
