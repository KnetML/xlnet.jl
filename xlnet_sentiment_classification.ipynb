{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrate how to train knet implementation of XLNet model for IMDB classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "└ @ CUDA /home/ec2-user/.julia/packages/CUDA/gKMm0/src/state.jl:251\n"
     ]
    }
   ],
   "source": [
    "include(\"./XLNet.jl\")\n",
    "using PyCall\n",
    "using DelimitedFiles\n",
    "using JLD2\n",
    "using Knet\n",
    "using Random\n",
    "using .XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = xlnet_base_hparams #xlnet_base_params are defined inside XLNet Module\n",
    "\n",
    "#Set sequance length and batch size accoring to your GPU\n",
    "SEQ_LEN = 340\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "#this hparam stands for how many layers to freeze from beggining.\n",
    "#You may play with this hparam according to your GPU memory. (There are tottaly 12 layers)\n",
    "hparams[\"n_freeze\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "For this example, we will work on sentiment classification on IMDB dataset (https://ai.stanford.edu/~amaas/data/sentiment/). Dataset can be downloaded as follows:\n",
    "\n",
    "```$ wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz```  \n",
    "```$ tar -xf aclImdb_v1.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/home/ec2-user/aclImdb\"\n",
    "\n",
    "#Read Train Data\n",
    "pos_train_files = readdir(dataset_path * \"/train/pos\")\n",
    "neg_train_files = readdir(dataset_path * \"/train/neg\")\n",
    "n_train = length(pos_train_files) + length(neg_train_files)\n",
    "n_classes = 2 #classes\n",
    "xtrn,ytrn = [],[]\n",
    "\n",
    "for filename in pos_train_files\n",
    "    s = open( dataset_path * \"/train/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_train_files\n",
    "    s = open( dataset_path * \"/train/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtrn,s)\n",
    "    push!(ytrn,\"neg\")\n",
    "end\n",
    "\n",
    "#Read Test Data\n",
    "pos_test_files = readdir(dataset_path * \"/test/pos\")\n",
    "neg_test_files = readdir(dataset_path * \"/test/neg\")\n",
    "n_train = length(pos_test_files) + length(neg_test_files)\n",
    "xtst,ytst = [],[]\n",
    "\n",
    "for filename in pos_test_files\n",
    "    s = open( dataset_path * \"/test/pos/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"pos\")\n",
    "end\n",
    "\n",
    "for filename in neg_test_files\n",
    "    s = open( dataset_path * \"/test/neg/\" * filename) do file; read(file, String); end\n",
    "    push!(xtst,s)\n",
    "    push!(ytst,\"neg\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize comments\n",
    "\n",
    "We will use sentencepiece tokenizer, you can download tokenizer model (```spiece.model```) from the following link:  \n",
    "  \n",
    "https://drive.google.com/file/d/1ZQMyHqnnBpQ_7H8SpJ205-0Nswmly1ME/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:24/00:24, 1029.85i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 901440.69i/s] \n",
      "┣                    ┫ [0.00%, 1/25000, 00:00/00:00, 70313.60i/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepearing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 00:25/00:25, 1009.35i/s] \n",
      "┣████████████████████┫ [100.00%, 25000/25000, 00:00/00:00, 878115.22i/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000-element Knet.Train20.Data{Tuple{Array{Int64,N} where N,Array{Int64,N} where N}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need to use sentencepice tokenizer from pycall\n",
    "#sentencepiece tokenizer : https://github.com/google/sentencepiece\n",
    "\n",
    "spm = pyimport(\"sentencepiece\")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"/home/ec2-user/project/checkpoint/spiece.model\")\n",
    "\n",
    "println(\"prepearing train data...\")\n",
    "xtrn = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtrn) ]\n",
    "ytrn = [ y==\"pos\" ? 2 : 1 for y in progress(ytrn) ]\n",
    "\n",
    "trn_token_ids = hcat( [x[1] for x in xtrn ]... )\n",
    "trn_seg_ids = hcat( [x[2] for x in xtrn ]... )\n",
    "trn_attn_masks= hcat( [x[3] for x in xtrn ]... )\n",
    "xtrn = cat(trn_token_ids,trn_seg_ids,trn_attn_masks,dims=3)\n",
    "xtrn = permutedims( xtrn, [1,3,2] )\n",
    "\n",
    "println(\"prepearing test data...\")\n",
    "xtst = [ prepare_sample( x, SEQ_LEN, sp ) for x in progress(xtst) ]\n",
    "ytst = [ y == \"pos\" ? 2 : 1 for y in progress(ytst) ]\n",
    "\n",
    "tst_token_ids = hcat( [x[1] for x in xtst ]... )\n",
    "tst_seg_ids = hcat( [x[2] for x in xtst ]... )\n",
    "tst_attn_masks= hcat( [x[3] for x in xtst ]... )\n",
    "xtst = cat(tst_token_ids, tst_seg_ids, tst_attn_masks, dims=3)\n",
    "xtst = permutedims( xtst, [1,3,2] )\n",
    "\n",
    "#Split Validation\n",
    "order = shuffle( collect(1:n_train) )\n",
    "xtrn = xtrn[:,:,order]\n",
    "ytrn = ytrn[order]\n",
    "\n",
    "\n",
    "nval = 2000\n",
    "xval = xtrn[:,:,1:nval]\n",
    "yval = ytrn[1:nval]\n",
    "xtrn = xtrn[:,:,nval+1:end]\n",
    "ytrn = ytrn[nval+1:end]\n",
    "\n",
    "dtrn = minibatch( xtrn, ytrn ,BATCH_SIZE, shuffle=true )\n",
    "dval = minibatch( xval, yval ,BATCH_SIZE, shuffle=true )\n",
    "dtst = minibatch( xtst, ytst ,BATCH_SIZE, shuffle=true )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define XLNet Model\n",
    "\n",
    "To be able to run finetuning, you need to download pretrained weights. I have prepared pretrained weights in JLD2 format. They can be downloaded from the following link:  \n",
    "  \n",
    "https://drive.google.com/file/d/1PpGnxQlJ6vcQ_78K7P0-dK-dL2qNU1A4/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier model created\n"
     ]
    }
   ],
   "source": [
    "#Load Weights;\n",
    "@load \"/home/ec2-user/xlnet_pretrained_base.jld2\" weights\n",
    "model = create_xlnet_model( hparams, weights )\n",
    "classifier = XLNetClassifier( hparams[\"d_model\"], n_classes , model )\n",
    "println(\"classifier model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training !\n",
    "\n",
    "Here we will train for 3 epoch, and save the best performing weights on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣██████████████▉     ┫ [74.56%, 17149/23000, 01:58:18/02:38:40, 2.52i/s] Excessive output truncated after 524324 bytes."
     ]
    }
   ],
   "source": [
    "trainer = adam( classifier, dtrn , lr = 1e-5 , eps=1e-8 )\n",
    "best_acc = 0\n",
    "for i=1:3\n",
    "    println(\"training epoch \",i)\n",
    "    progress!(trainer)\n",
    "    acc = acc = accuracy( classifier ,  progress( dval ) )\n",
    "    if( acc > best_acc )\n",
    "        best_acc = acc\n",
    "        save(\"best.jld2\", classifier ) #pretty easy saving :)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test !\n",
    "\n",
    "Load the saved weights  and test on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┣████████████████████┫ [100.00%, 25000/25000, 01:33:10/01:33:10, 4.47i/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.93348\n"
     ]
    }
   ],
   "source": [
    "classifier = XLNetClassifier(\"best.jld2\") #Load model from saved path\n",
    "acc = accuracy( classifier ,  progress( dtst ) )\n",
    "println(\"accuracy : \" , acc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "Here we can classify custom comment about _Queen's Gambit_ series:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x) = x[1] > x[2] ? println(\"Negative\") : println(\"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "comment = \"When you've seen everything on Netflix, a you will watch any new show they throw at you. I did not have high expectations. But I was so wrong. So wonderfully and totally wrong. We watched Queen's Gambit, all seven hours of it, in a single sitting. This show is an absolute gem. Remarkable acting by Anya Taylor-Joy who will be a force to be reckoned with, her child counterpart- an absolute marvel. The series is smart, it's inspiring at times, it's sad and heartbreaking at times. Who knew watching game-plays of simulated chess can make your heart pound out of your ears? Truly, a story of determination, friendship, self discovery, struggle and triumph. I don't know about you, but my ceiling will be covered in chess pieces tonight.\"\n",
    "\n",
    "trn_token_ids, trn_seg_ids, trn_attn_masks = prepare_sample( comment, SEQ_LEN, sp )\n",
    "ids = hcat(trn_token_ids, trn_seg_ids, trn_attn_masks  )\n",
    "ids = reshape(ids, (size(ids)...,1))\n",
    "\n",
    "predict( classifier(ids) )\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
